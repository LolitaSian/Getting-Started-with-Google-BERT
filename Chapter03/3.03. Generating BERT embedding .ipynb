{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LolitaSian/Getting-Started-with-Google-BERT/blob/main/Chapter03/3.03.%20Generating%20BERT%20embedding%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOaCES6J_FKK"
      },
      "source": [
        "# Hugging Face transformers \n",
        "\n",
        "Hugging Face is an organization that is on a path to solve and democratize AI through natural language. Their open-source library 'transformers'  is very popular among the NLP community. It is very useful and powerful for several NLP and NLU tasks. It includes thousands of pre-trained models in about 100+ languages. One of the many advantages of the transformer library is that it is compatible with both PyTorch and TensorFlow.\n",
        "\n",
        "We can install transformers directly using pip as shown in the following: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "o7PQZSHt_FKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924aaa21-b009-4773-bbe0-18daefda33a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers \n",
        "# 有原来那句会跑不通，并且指定版本之后会出现奇怪bug，可能是两年前代码太老了。遂改之。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ehF5uSR_FKZ"
      },
      "source": [
        "\n",
        "As we can see, in this book, we use transformers version 3.5.1. Now that we installed transformers, let's get started. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi1Nq51R_FKc"
      },
      "source": [
        "# Generating BERT embedding \n",
        "In this section, we will learn how to extract embeddings from the pre-trained BERT. Consider the sentence 'I love Paris'. Let's see how to obtain the contextualized word embedding of all the words in the sentence using the pre-trained BERT model with Hugging Face's transformer library. \n",
        "\n",
        "First, let's import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "id": "qmWTWzRX_FKe"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFPzX86E_FKh"
      },
      "source": [
        "Next, we download the pre-trained BERT model. We can check all the available pre-trained BERT models here - https://huggingface.co/transformers/pre-trained_models.html.We use the 'bert-base-uncased' model. As the name suggests, it is the BERT-base model with 12 encoders and it is trained with uncased tokens. Since we are using the BERT-base, the representation size will be 768. \n",
        "\n",
        "Download and load the pre-trained bert-base-uncasedmodel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swqqjz08_FKj",
        "outputId": "f13c85d4-e782-4856-873e-cad81040ea97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KODXve4P_FKl"
      },
      "source": [
        "Next, we download and load the tokenizer which is used for pretraining the bert-base-uncased model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lPHa9dI5_FKn"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIHYHG5A_FKr"
      },
      "source": [
        "\n",
        "Now, let's see how to preprocess the input before feeding it to the BERT. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8El2dcG_FKs"
      },
      "source": [
        "## Preprocessing the input \n",
        "Define the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": true,
        "id": "8Hkc5o3B_FKu"
      },
      "outputs": [],
      "source": [
        "sentence = 'I love Paris'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVn3-2XM_FKv"
      },
      "source": [
        "Tokenize the sentence and obtain the tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "4Ui-3oWr_FKw"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSYRLcvV_FKy"
      },
      "source": [
        "Let's print the tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGHtPj5H_FKz",
        "outputId": "8591fd61-d694-4ad1-eff9-39c2824c843e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'love', 'paris']\n"
          ]
        }
      ],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL8VgpcM_FK0"
      },
      "source": [
        "Now, we will add the [CLS] token at the beginning and [SEP] token at the end of the tokens list: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": true,
        "id": "3CpARvDc_FK0"
      },
      "outputs": [],
      "source": [
        "tokens = ['[CLS]'] + tokens + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbFEyrk7_FK2"
      },
      "source": [
        "Let's look at our updated tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaf_JScw_FK3",
        "outputId": "94300d9d-0ab6-4214-9aae-039f0c0cf56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'love', 'paris', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVD29L7A_FK3"
      },
      "source": [
        "As we can observe, we have [CLS] token at the beginning and sep token at the end of our tokens list. We can also observe that length of our tokens is 5.\n",
        "\n",
        "Say, we need to keep the length of our tokens list to 7, then, in that case, we will add two [PAD] tokens at the end as shown in the following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "cqMI--rA_FK4"
      },
      "outputs": [],
      "source": [
        "tokens = tokens + ['[PAD]'] + ['[PAD]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEEHs7CV_FK6"
      },
      "source": [
        "Let's print our updated tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tKmh_hk_FK6",
        "outputId": "53092a22-6044-4852-da37-e33af64db13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']\n"
          ]
        }
      ],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPd9TPWE_FK7"
      },
      "source": [
        "\n",
        "\n",
        "As we can observe, now we have the tokens list consists of [PAD] tokens and the length of our tokens list is 7. \n",
        "\n",
        "Next, we create the attention mask. We set the attention mask value to 1 if the token is not a [PAD] token else we will set the attention mask to 0 as shown below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "3SbiaBws_FK8"
      },
      "outputs": [],
      "source": [
        "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SELKgxsG_FK9"
      },
      "source": [
        "Let's print the attention_mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmtZKFFq_FK-",
        "outputId": "b1c840d9-2de6-4516-8620-fb70f97bbad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvtvC8HW_FK-"
      },
      "source": [
        "\n",
        "As we can observe, we have attention mask values 0 at the position where have [PAD] token and 1 at other positions. \n",
        "\n",
        "Next, we convert all the tokens to their token_ids as shown below: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "id": "KGbsHfxL_FK_"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQWwz_y_FK_"
      },
      "source": [
        "\n",
        "Let's have a look at the token_ids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iih_wWeT_FLA",
        "outputId": "1b8ae455-61aa-4053-e4ce-673c11c7d1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1045, 2293, 3000, 102, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9RL1cBs_FLA"
      },
      "source": [
        "\n",
        "From the above output, we can observe that each token is mapped to a unique token id.\n",
        "\n",
        "Now, we convert the token_ids and attention_mask to tensors as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "KQzsDUez_FLB"
      },
      "outputs": [],
      "source": [
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQn5de_a_FLC"
      },
      "source": [
        "\n",
        "That's it. Next, we feed the token_ids and attention_mask to the pre-trained BERT model and get the embedding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYIDWAO-_FLD"
      },
      "source": [
        "## Getting the embedding \n",
        "\n",
        "As shown in the following code, we feed the token_ids, and attention_mask to the model and get the embeddings. Note that the model returns the output as a tuple with two values. The first value indicates the hidden state representation, hidden_rep and it consists of the representation of all the tokens obtained from the final encoder (encoder 12), and the second value, cls_head consists of the representation of the [CLS] token: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "Rh-Ohh71_FLE"
      },
      "outputs": [],
      "source": [
        "output = model(token_ids, attention_mask = attention_mask)\n",
        "# 让它自动解包会出问题，所以改了\n",
        "hidden_rep = output.last_hidden_state\n",
        "cls_head = output.pooler_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Kfvd3F_FLF"
      },
      "source": [
        "In the preceding code, hidden_rep contains the embedding(representation) of all the tokens in our input. Let's print the shape of hidden_rep tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx1RfsaU_FLF",
        "outputId": "ce249d2f-d2aa-4065-e219-58e814942997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ]
        }
      ],
      "source": [
        "print(hidden_rep.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWxFowIS_FLG"
      },
      "source": [
        "The size [1,7,768] indicates the[batch_size, sequence_length, hidden_size].\n",
        "\n",
        "Our batch size is 1, the sequence length is the token length, since we have 7 tokens, the sequence length is 7, and the hidden size is the representation (embedding) size and it is 768 for the BERT-base model. \n",
        "\n",
        "We can obtain the representation of each token as: \n",
        "\n",
        "- hidden_rep[0][0] gives the representation of the first token which is [CLS]\n",
        "- hidden_rep[0][1] gives the representation of the second token which is 'I' \n",
        "- hidden_repo[0][2] gives the representation of the third token which is 'love' \n",
        "\n",
        "In this way, we can obtain the contextual representation of all the tokens. This is basically the contextualized word embeddings of all the words in the given sentence. \n",
        "\n",
        "Now, let's take a look at the cls_head. It contains the representation of the [CLS] token. Let's print the shape of cls_head :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAL6KEXw_FLI",
        "outputId": "029dc79c-3926-400e-ee88-5b97930d1ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 768])\n"
          ]
        }
      ],
      "source": [
        "print(cls_head.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL8eMwZY_FLJ"
      },
      "source": [
        "The above code will print:\n",
        "\n",
        "torch.Size([1,768])\n",
        "\n",
        "The size [1,768] indicates the[batch_size, hidden_size].\n",
        "\n",
        "We learned that cls_head holds the aggregate representation of the sentence, so we can use the cls_head as the representation of the given sentence 'I love Paris'. \n",
        "\n",
        "We learned how to extract embeddings from the pre-trained BERT. But these are the embeddings obtained only from the topmost encoder layer of BERT which is encoder 12. Can we also extract the embeddings from all the encoder layers of BERT? Yes! We will find out how to do that in the next section. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "3.03. Generating BERT embedding .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}