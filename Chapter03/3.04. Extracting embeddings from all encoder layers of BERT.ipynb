{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LolitaSian/Getting-Started-with-Google-BERT/blob/main/Chapter03/3.04.%20Extracting%20embeddings%20from%20all%20encoder%20layers%20of%20BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLbXrzA0_ndC"
      },
      "source": [
        "# Extracting embeddings from all encoder layers of BERT\n",
        "We learned how to extract the embedding from the pre-trained BERT in the previous section. We learned that they are the embeddings obtained from the final encoder layer. Now the question is should we consider the embedding obtained only from the final encoder layer (final hidden state), or should we also consider the embedding obtained from all the encoder layers (all hidden states)? Let's explore more about this. \n",
        "\n",
        "Let us represent the input embedding layer by $h_0$  and the first encoder layer (first hidden layer) by $h_1$, second encoder layer (second hidden layer) by $h_2$ and so on to the final twelfth encoder layer by $h_{12}$ as shown in the following figure:\n",
        "\n",
        "\n",
        "![title](https://github.com/LolitaSian/Getting-Started-with-Google-BERT/blob/main/Chapter03/images/4.png?raw=1)\n",
        "\n",
        "\n",
        "Instead of taking the embeddings (representation) only from the final encoder layer, the researchers of the BERT have experimented with taking embeddings from different encoder layers.\n",
        "\n",
        "For instance, for a named-entity recognition task, the researchers have used the pre-trained BERT for extracting features. Instead of using the embedding only from the final encoder layer (final hidden layer) as a feature, they have experimented using embedding  from other encoder layers (other hidden layers) as a feature and obtained the following F1 score: \n",
        "\n",
        "\n",
        "![title](https://github.com/LolitaSian/Getting-Started-with-Google-BERT/blob/main/Chapter03/images/5.png?raw=1)\n",
        "\n",
        "As we can observe from the preceding table, concatenating the embeddings of the last 4 encoder layers (last 4 hidden layers) gives us a greater F1 score of 96.1% in the  NER task. Thus, instead of taking the embeddings only from the final encoder layer (final hidden layer), we can also use embeddings from the other encoder layers.\n",
        "\n",
        "Now, we will learn how to extract the embeddings from all the encoder layers using the transformers library. \n",
        "\n",
        "## Extracting the embeddings \n",
        "First, let us import the necessary modules: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "A9uDL3-i_rG-",
        "outputId": "11806614-9521-4233-8df1-de258aee7195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "id": "KSwq5tr9_ndJ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI91BJ8H_ndJ"
      },
      "source": [
        "\n",
        "Next, download the pre-trained BERT model and tokenizer. As we can notice while downloading the pre-trained BERT model. We need to set output_hidden_states = True. By setting this to true helps us to obtain embeddings from all the encoder layers: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM0UkRBp_ndK",
        "outputId": "ee5828e4-0f45-4881-cb6c-80f660290671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6lcm9mn_ndK"
      },
      "source": [
        "\n",
        "Next, we preprocess the input before feeding it to the model. \n",
        "\n",
        "## Preprocess the input\n",
        "Let's consider the same sentence we saw in the previous section. First, we tokenize the sentence and add [CLS] token at the beginning and [SEP] token at the end: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "1-hLzFu6_ndL"
      },
      "outputs": [],
      "source": [
        "sentence = 'I love Paris'\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUKVAdmT_ndL"
      },
      "source": [
        "\n",
        "Suppose, we need to keep the token length to 7. So, we add the [PAD] tokens and also define the attention mask: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "5OISI_-t_ndM"
      },
      "outputs": [],
      "source": [
        "tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
        "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic5yVHnB_ndM"
      },
      "source": [
        "\n",
        "Next, we convert the tokens to their token_ids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "8YtAw5Gr_ndM"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MmpLWP6_ndN"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Now, we convert the token_ids and attention_mask to tensor: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "lm7Nqj6w_ndN"
      },
      "outputs": [],
      "source": [
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FQ7RzxU_ndN"
      },
      "source": [
        "\n",
        "\n",
        "Now that we preprocessed the input, let's get the embedding. \n",
        "\n",
        "## Getting the embedding \n",
        "Since we set output_hidden_states = True while defining the model for getting the embeddings from all the encoder layers, now the model returns an output tuple with three values as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "id": "DhmqjLG1_ndN"
      },
      "outputs": [],
      "source": [
        "output = model(token_ids, attention_mask = attention_mask)\n",
        "last_hidden_state = output.last_hidden_state\n",
        "pooler_output = output.pooler_output\n",
        "hidden_states = output.hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjN7Ojnh_ndO"
      },
      "source": [
        "\n",
        "In the preceding code, the following applies: \n",
        "\n",
        "The first value last_hidden_state contains the representation of all the tokens obtained only from the final encoder layer (encoder 12). \n",
        "Next, pooler_output indicates the representation of the [CLS] token from the final encoder layer which is further processed by a linear and tanh activation function. \n",
        "hidden_states contains the representation of all the tokens obtained from all the final encoder layers. \n",
        "Now, let us take a look into each of these values and understand them in more detail. \n",
        "\n",
        "First, let us look at last_hidden_state. As we learned, it holds the representation of all the tokens obtained only from the final encoder layer (encoder 12). Let us print the shape of the last_hidden_state: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlmKmXrO_ndO",
        "outputId": "1c871ddc-66b5-4dfa-867b-422401a2badc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCSrepOZ_ndO"
      },
      "source": [
        "\n",
        "The size [1,7,768] indicates the[batch_size, sequence_length, hidden_size].\n",
        "\n",
        "Our batch size is 1, the sequence length is the token length and since we have 7 tokens the sequence length is 7, and the hidden size is the representation (embedding) size and it is 768 for the BERT-base model. \n",
        "\n",
        "We can obtain the embedding of each token as: \n",
        "- last_hidden[0][0] gives the representation of the first token which is [CLS]\n",
        "- last_hidden[0][1] gives the representation of the second token which is 'I' \n",
        "- last_hidden[0][2] gives the representation of the third token which is 'love' \n",
        "\n",
        "Similarly, we can obtain the representation of all the tokens from the final encoder layer. \n",
        "\n",
        "Next, we have pooler_output which contains the representation of the [CLS] token from the final encoder layer which is further processed by a linear and tanh activation function. Let us print the shape of the pooler_output: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5MrEuW_ndP",
        "outputId": "7dc5175b-ccb2-4929-ec06-43d05fc6c6e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "pooler_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERlJpFL3_ndP"
      },
      "source": [
        "\n",
        "The size [1,768] indicates the[batch_size, hidden_size].\n",
        "\n",
        "We learned that [CLS] token holds the aggregate representation of the sentence. Thus, we can use the pooler_output as the representation of the given sentence 'I love Paris'. \n",
        "\n",
        "Finally, we have hidden_states and it contains the representation of all the tokens obtained from all the final encoder layers. It is a tuple containing 13 values holding the representation of all encoder layers (hidden layers) starting from the input embedding layer  to the final encoder layer . \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzF8Qtxu_ndP",
        "outputId": "8dec6b16-f836-426f-dc5f-b7a977018395"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "len(hidden_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBdQEeI_ndP"
      },
      "source": [
        "\n",
        "As we can notice, it contains 13 values holding the representation of all layers. Thus: \n",
        "\n",
        "- hidden_states[0] contains the representation of all the tokens obtained from the input embedding layer \n",
        "- hidden_states[1] contains the representation of all the tokens obtained from the first encoder layer \n",
        "- hidden_states[2] contains the representation of all the tokens obtained from the second encoder layer \n",
        "\n",
        "Similarly, hidden_states[12] contains the representation of all the tokens obtained from the final encoder layer \n",
        "Let's explore this more. First, let's print the shape of the hidden_states[0] which contains the representation of all the tokens obtained from the input embedding layer : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUT57PxO_ndQ",
        "outputId": "f6f443eb-a1dd-4ce9-d7ca-e7c34a99db86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "hidden_states[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPc_0HL3_ndQ"
      },
      "source": [
        "\n",
        "The size [1,7,768] indicates the[batch_size, sequence_length, hidden_size].\n",
        "\n",
        "Now, let's print the shape of hidden_states[1] which contains the representation of all tokens obtained from the first encoder layer : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yGnFpzH_ndR",
        "outputId": "830c8177-42c2-467d-d932-1a7bd04d4925"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "torch.Size([1, 7, 768])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy_0LBQi_ndR",
        "outputId": "60f8e06a-5e6f-41c6-ce0c-d138e2178f56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "hidden_states[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46j8VT75_ndR"
      },
      "source": [
        "\n",
        "Thus, in this way, we can obtain the embedding of tokens from all the encoder layers. We learned how to use the pre-trained BERT to extract embeddings, can we also use pre-trained BERT for a downstream task like sentiment analysis? Yes! We will learn about this in the next section. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "3.04. Extracting embeddings from all encoder layers of BERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}